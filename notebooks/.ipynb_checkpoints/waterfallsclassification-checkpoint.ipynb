{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dropout, Add, Dense, Reshape, Activation\n",
    "from keras.layers import BatchNormalization, Flatten, Conv2D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Flow Tracking Server\n",
    "\n",
    "I have ML Flow running locally.  When I start it it exports the IP and port it will use.  Here, we try to read those in and track experiments if the ML Flow server is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_IP = os.getenv('MLFLOW_IP')\n",
    "MLFLOW_PORT = os.getenv('MLFLOW_PORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"ResNet for Waterfall Plots\"\n",
    "try:\n",
    "    mlflow.set_experiment(name)\n",
    "except Exception as e:\n",
    "    print(\"Exception using set_experiment\")\n",
    "\n",
    "try:\n",
    "    mlflow.start_run()\n",
    "    print(f\"started run with name: {name}\")\n",
    "except Exception as e:\n",
    "    print(\"Exception using start_run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.autolog()\n",
    "print(\"Setup ML Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Config\n",
    "\n",
    "Check that it's available, and configure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs detected:\", gpus)\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "count = 50\n",
    "counter = 0\n",
    "DATASET_ROOT_DIR = './datasets'\n",
    "for dirname, _, filenames in os.walk(DATASET_ROOT_DIR):\n",
    "    for filename in filenames:        \n",
    "        counter += 1\n",
    "        if counter % 30 == 0:\n",
    "            counter = 0\n",
    "            img = plt.imread(dirname + '/' + filename)\n",
    "            print(dirname + '/' + filename, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './datasets/automatic-picture-transmission/f7a3be24e8b2f2755e07bd98b474f09e.png'\n",
    "IMGSIZE=(128,128)\n",
    "img = cv2.resize(cv2.imread(file), IMGSIZE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './datasets/lora/40fd4bd65f1e7dc0a7240387ca96cefb.png'\n",
    "IMGSIZE=(128,128)\n",
    "img = cv2.resize(cv2.imread(file), IMGSIZE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './datasets/vor/fd6e4c3603fccd0ae9e611350e7e284a.png'\n",
    "IMGSIZE=(128,128)\n",
    "img = cv2.resize(cv2.imread(file), IMGSIZE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
    "    DATASET_ROOT_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=.2,\n",
    "    subset=\"both\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "\n",
    "        # First convolution\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            strides=strides,\n",
    "            padding='same', # Use 'same' padding to maintain spatial dimensions\n",
    "            use_bias=False \n",
    "        )\n",
    "        # Batch Normalization after convolution\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        # Activation (usually ReLU)\n",
    "        self.relu = layers.Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply the layers sequentially\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the Identity Block (Residual Block)\n",
    "# This block is used when the input and output shapes of the block\n",
    "# are the same, allowing the shortcut connection to be a simple identity mapping.\n",
    "class IdentityBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Store the input for the shortcut connection\n",
    "        shortcut = inputs\n",
    "\n",
    "        # Main Path\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = layers.Add()([x, shortcut])\n",
    "\n",
    "        # Apply final activation after the addition\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class SimpleResNet(keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initial Convolution and Pooling\n",
    "        self.conv1 = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "        self.max_pool = layers.MaxPooling2D(3, strides=2, padding='same')\n",
    "\n",
    "        self.id_block1 = IdentityBlock(filters=64, kernel_size=3)\n",
    "        self.id_block2 = IdentityBlock(filters=64, kernel_size=3)\n",
    "\n",
    "        # A convolution block to change dimensions, followed by identity blocks\n",
    "        # self.conv_block1 = ConvolutionBlock(filters=128, kernel_size=3, strides=2) # Stride 2 reduces spatial size\n",
    "        # self.id_block3 = IdentityBlock(filters=128, kernel_size=3)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
    "\n",
    "        # Dense layer for classification\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax') # Use softmax for multi-class output\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs should be (batch_size, 256, 256, 3)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Apply residual blocks\n",
    "        x = self.id_block1(x)\n",
    "        x = self.id_block2(x)\n",
    "\n",
    "        # If using convolution block:\n",
    "        # x = self.conv_block1(x)\n",
    "        # x = self.id_block3(x)\n",
    "\n",
    "        # Final layers\n",
    "        x = self.global_avg_pool(x)\n",
    "        outputs = self.classifier(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(layers.Layer):\n",
    "    # Keep ConvolutionBlock as is, its layers are simple and built on first call\n",
    "    def __init__(self, filters, kernel_size, strides=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "\n",
    "        # First convolution\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Batch Normalization after convolution\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        # Activation (usually ReLU)\n",
    "        self.relu = layers.Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class IdentityBlock(layers.Layer):\n",
    "    # Keep IdentityBlock as is, its layers are simple and built on first call\n",
    "    def __init__(self, filters, kernel_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shortcut = inputs\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = layers.Add()([x, shortcut])\n",
    "\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleResNet(keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # Layers are now created in the build method\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape will be something like (None, 256, 256, 3)\n",
    "\n",
    "        # Initial Convolution and Pooling\n",
    "        self.conv1 = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False, name='conv1')\n",
    "        self.bn1 = layers.BatchNormalization(name='bn1')\n",
    "        self.relu1 = layers.Activation('relu', name='relu1')\n",
    "        self.max_pool = layers.MaxPooling2D(3, strides=2, padding='same', name='max_pool')\n",
    "\n",
    "        # Residual Blocks\n",
    "        # Pass the filters explicitly here\n",
    "        self.id_block1 = IdentityBlock(filters=64, kernel_size=3, name='id_block1')\n",
    "        self.id_block2 = IdentityBlock(filters=64, kernel_size=3, name='id_block2')\n",
    "\n",
    "        self.conv_block1 = ConvolutionBlock(filters=128, kernel_size=3, strides=2, name='conv_block1')\n",
    "        self.id_block3 = IdentityBlock(filters=128, kernel_size=3, name='id_block3')\n",
    "\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling2D(name='global_avg_pool')\n",
    "\n",
    "        # Dense layer for classification\n",
    "        self.classifier = layers.Dense(self.num_classes, activation='softmax', name='classifier') # Use softmax for multi-class output\n",
    "\n",
    "        # It's good practice to call the parent build method\n",
    "        super().build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs should be (batch_size, 256, 256, 3)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Apply residual blocks\n",
    "        x = self.id_block1(x)\n",
    "        x = self.id_block2(x)\n",
    "\n",
    "        # If using convolution block:\n",
    "        # x = self.conv_block1(x)\n",
    "        # x = self.id_block3(x)\n",
    "\n",
    "        # Final layers\n",
    "        x = self.global_avg_pool(x)\n",
    "        outputs = self.classifier(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "batch = 32\n",
    "\n",
    "callbacks_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ELEMENT SPEC:\", train_ds.element_spec)\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"  batch x:\", images.shape, type(images), \n",
    "          \"\\n  batch y:\", labels, labels.shape if labels is not None else None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=20\n",
    "inputs = keras.Input((256,256,3))\n",
    "outputs = SimpleResNet(num_classes)(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleResNet(num_classes)\n",
    "\n",
    "model.build((None, 256, 256, 3)) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "## \"devices=None\" uses all GPUs if they're detected, or falls back to CPU if no GPUs detected\n",
    "strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "with strategy.scope():\n",
    "    model = SimpleResNet(num_classes=20)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "start = time.time()\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=val_ds)\n",
    "print(f'Totally fit time: {time.time() - start:<10.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"val_loss\", val_loss)\n",
    "    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "    \n",
    "    adm = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)\n",
    "    num_epochs = 200\n",
    "    batch = 32\n",
    "    callbacks_list = []\n",
    "    num_classes=20\n",
    "    inputs = keras.Input((256,256,3))\n",
    "    outputs = SimpleResNet(num_classes)(inputs)\n",
    "    model = keras.Model(inputs, outputs) \n",
    "    model = SimpleResNet(num_classes)\n",
    "\n",
    "    model.build((None, 256, 256, 3)) \n",
    "    model.summary()\n",
    "    ## \"devices=None\" uses all GPUs if they're detected, or falls back to CPU if no GPUs detected\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    with strategy.scope():\n",
    "        model = SimpleResNet(num_classes=20)\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=num_epochs,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=val_ds)\n",
    "    print(f'Totally fit time: {time.time() - start:<10.5f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5853248,
     "sourceId": 9595609,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
